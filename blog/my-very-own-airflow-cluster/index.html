<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><base href=https://zmsy.co/><title>zmsy | My Very Own Airflow Cluster</title><meta name=og:title content="My Very Own Airflow Cluster"><meta name=description content="My data needs have expanded over the years, and it took a full Airflow setup to finally rise to the occasion."><meta name=og:description content="My data needs have expanded over the years, and it took a full Airflow setup to finally rise to the occasion."><link rel=stylesheet href=https://zmsy.co/css/main.css><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css><link href="https://fonts.googleapis.com/css?family=Patua+One|Work+Sans&display=swap" rel=stylesheet><link rel=icon type=image/png href=https://zmsy.co/media/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://zmsy.co/media/favicon-16x16.png sizes=16x16><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zmsy.co\/"},"name":"My Very Own Airflow Cluster","headline":"My Very Own Airflow Cluster","description":"My data needs have expanded over the years, and it took a full Airflow setup to finally rise to the occasion.","inLanguage":"en-US","author":"Zach Morrissey","creator":"Zach Morrissey","publisher":"Zach Morrissey","accountablePerson":"Zach Morrissey","copyrightHolder":"Zach Morrissey","copyrightYear":"2022","datePublished":"2018-01-31","dateModified":"2022-01-28T03:57:47Z","url":"https:\/\/zmsy.co\/blog\/my-very-own-airflow-cluster\/","keywords":["Blog"]}</script></head><body><div class="columns is-centered" id=navbar-columns><div class="column is-10 is-12-touch" id=navbar-column><nav class=navbar role=navigation id=top><div class=navbar-brand><a class=navbar-item href=https://zmsy.co/><img id=logo-img src=https://zmsy.co/media/logo.svg alt="zmsy.co - Home"></a>
<a class=navbar-item href=https://zmsy.co/><div class=is-3 id=logo-text>zmsy.co</div></a><span class=navbar-burger id=navbar-toggle><span></span><span></span><span></span></span></div><div class=navbar-menu><div class=navbar-end><a class="navbar-item is-hidden-touch" href=https://www.linkedin.com/in/zmsyco><span class=icon><i class="fa fa-linkedin-square" title="View my LinkedIn profile."></i></span></a><a class="navbar-item is-hidden-touch" href=https://github.com/zmsy><span class=icon><i class="fa fa-github" title="Check out my Github."></i></span></a><a class="navbar-item is-hidden-touch" href=https://twitter.com/_zmsy><span class=icon><i class="fa fa-twitter" title="Check out my sparsely used Twitter account."></i></span></a><a class="navbar-item is-tab" href=https://zmsy.co/>Home</a>
<a class="navbar-item is-tab" href=https://zmsy.co/blog>Blog</a>
<a class="navbar-item is-tab" href=https://zmsy.co/projects>Projects</a>
<a class="navbar-item is-tab" href=https://zmsy.co/about>About</a></div></div></nav></div></div><div id=body><div class="columns is-centered is-desktop"><div class="column is-8 is-12-touch"><section class=section><div class=blog-post><h2><a class="title title-dark is-size-2-touch" href=https://zmsy.co/blog/my-very-own-airflow-cluster/>My Very Own Airflow Cluster</a></h2><p class=meta>written by
<a href=https://twitter.com/_zmsy>Zach Morrissey</a>
on 2018-06-25</p><img class=image id=hero-diamonds src=https://zmsy.co/img/diamonds-dark.svg><div class=content><p>As someone who writes a lot of one-off batch scripts, the rise of DAG (Directed Acyclic Graph) task engines has made my life a <em>lot</em> better. I&rsquo;ve used <a href=https://github.com/spotify/luigi>Luigi</a> at work for about 2 years now. For feeding the backend of this website with data, I decided to set up an Airflow cluster.</p><figure><img src=https://zmsy.co/blog/my-very-own-airflow-cluster/airflow.png alt="Airflow screenshot"></figure><p>As opposed to this paradigm:</p><ul><li>Write a script.</li><li>Set it up in <code>cron</code>.</li><li>Check the logs whenever something goes wrong.</li></ul><p>You get a whole lot more bang for your buck, with similar amounts of work. DAG paradigm is more like:</p><ul><li>Write a script.</li><li>Write an airflow DAG, where you call that script.</li><li>Set a schedule for the script.</li><li>Check on it from the built-in Web UI.</li><li>Stop and restart tasks whenever you want to.</li><li>View the console output from the Web UI.</li></ul><p>It <em>really</em> helps certain types of batch processes scale past a certain point, while simplifying the process of managing and deploying them. You can relatively easily set up Airflow to schedule thousands of tasks without all that much more configuration past what you&rsquo;d normally write.</p><h2 id=the-good-and-bad-reasons-to-use-airflow>The Good and Bad Reasons to Use Airflow</h2><p>There&rsquo;s a number of gotchas you should know prior to using one of the DAG engines, as they all fall into the same traps.</p><ul><li><code>cron</code> is <em>incredibly</em> difficult to beat for reliability. There&rsquo;s a reason it&rsquo;s ubiquitous. DAG engines are usually for tasks that need to scale past what <code>cron</code> is built for. If you&rsquo;re just trying to run a single script on the regular, stick with that instead.</li><li>DAG engines are meant for batch programs. If you need anything real-time, look elsewhere.</li><li>Usually there&rsquo;s a non-trivial amount of setup involved at the beginning.</li><li>Sometimes scaling can be a pain. For Airflow, you&rsquo;ll need to setup a task queue like Celery and a number of different nodes (web server, database, etc) once you scale past a certain point.</li><li>There&rsquo;s a <a href=https://github.com/puckel/docker-airflow>relatively mature Docker image</a> that you can reuse if you don&rsquo;t want to configure it yourself.</li></ul><h2 id=diving-in>Diving In</h2><p>There are plenty of ways that you can configure Airflow. There are varying levels of complexity for varying levels of requirements, specified by the Executor types (i.e. what does what in your scenario).</p><ul><li><strong>SequentialExecutor</strong> - Uses SQLite as a backend, and executes tasks for testing/debugging. This is generally what you&rsquo;d run on your own machine if you&rsquo;re just testing a workflow.</li><li><strong>LocalExecutor</strong> - Threaded local executor that uses just a DB connection as a backend.</li><li><strong>CeleryExecutor</strong> - For distributing tasks out to worker nodes. This uses Celery as a queue. Requires you to set up other nodes as worker nodes.</li></ul><p>Here&rsquo;s diagram of how it works.</p><figure><img src=https://zmsy.co/blog/my-very-own-airflow-cluster/airflow-diagram.svg alt="Airflow architecture" class=no-border></figure><p>For my purposes, I went with LocalExecutor, and set up the configuration in the outlined area above. I set up two nodes for this in my <a href=https://www.proxmox.com/en/>Proxmox cluster</a>.</p><ol><li><strong>Airflow web server.</strong> This was just based on a Ubuntu 16.04 instance.</li><li><strong>Postgres backend.</strong> For the LocalExecutor option, this is the only additional node required.</li></ol><p>The LocalExecutor option (configured for using Postgres as a backend in this instance) has probably the highest payoff-to-effort ratio, compared to how difficult it can be to set up the CeleryExecutor. Once you&rsquo;ve created the database, verified the connection works, and run <code>airflow initdb</code>, you&rsquo;re good to go.</p><p>Now you can start making DAGs.</p><h2 id=dags-operators-upstream-huh>DAGS, Operators, Upstream, Huh?</h2><p>There&rsquo;s <a href=https://airflow.incubator.apache.org/concepts.html>a lot of lingo to be learned</a> when using Airflow.</p><ul><li><strong>DAG</strong> - This is a single workflow, wherein you can arrange tasks and dependencies.</li><li><strong>Operator</strong> - This is a single unit of work that can be instantiated multiple times to achieve a particular output/goal/etc. There are things like <code>BashOperator</code> for executing shell scripts, or <code>PythonOperator</code> for python files, etc.</li><li><strong>DAG Run</strong> - A single execution of a DAG of tasks. Each operator in the DAG is instantiated and executed as it&rsquo;s dependencies are completed.</li></ul><h3 id=example-dag>Example DAG</h3><p>Fittingly, the DAG is a graph, both directed and acyclic. Task execution flows in only one direction.</p><figure><img src=https://zmsy.co/blog/my-very-own-airflow-cluster/airflow-dag.svg alt="An Airflow dag instance" class=no-border></figure><ul><li>The arrows indicate execution order.</li><li>An arrow from task 1 to task 2 implies task 1 is a dependency for task 2. If task 1 fails, task 2 will not execute.</li><li>Nodes can have multiple dependencies, like node 4 above.</li><li>Nodes can have multiple downstream tasks, like nodes 1 and 3.</li><li>Task execution timing is non-deterministic, so there&rsquo;s no guarantee for whether sibling tasks 2 or 3 would execute first.</li></ul><h3 id=in-code>In Code</h3><p>Here&rsquo;s some example statements that would make a dag resembling the one in the graphic above.</p><div class=highlight><pre style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>default_args <span style=color:#555>=</span> {owner<span style=color:#555>=</span><span style=color:#c30>&#39;airflow&#39;</span>}  <span style=color:#09f;font-style:italic># server user account where this is run</span>
dag <span style=color:#555>=</span> DAG(
    <span style=color:#c30>&#39;example&#39;</span>, default_args<span style=color:#555>=</span>default_args)
t1 <span style=color:#555>=</span> BashOperator(task_id<span style=color:#555>=</span><span style=color:#c30>&#39;t1&#39;</span>, bash_command<span style=color:#555>=</span><span style=color:#c30>&#39;echo &#34;task 1&#34;&#39;</span>, dag<span style=color:#555>=</span>dag)
t2 <span style=color:#555>=</span> BashOperator(task_id<span style=color:#555>=</span><span style=color:#c30>&#39;t2&#39;</span>, bash_command<span style=color:#555>=</span><span style=color:#c30>&#39;echo &#34;task 2&#34;&#39;</span>, dag<span style=color:#555>=</span>dag)
t3 <span style=color:#555>=</span> BashOperator(task_id<span style=color:#555>=</span><span style=color:#c30>&#39;t3&#39;</span>, bash_command<span style=color:#555>=</span><span style=color:#c30>&#39;echo &#34;task 3&#34;&#39;</span>, dag<span style=color:#555>=</span>dag)
t4 <span style=color:#555>=</span> BashOperator(task_id<span style=color:#555>=</span><span style=color:#c30>&#39;t4&#39;</span>, bash_command<span style=color:#555>=</span><span style=color:#c30>&#39;echo &#34;task 4&#34;&#39;</span>, dag<span style=color:#555>=</span>dag)
t5 <span style=color:#555>=</span> BashOperator(task_id<span style=color:#555>=</span><span style=color:#c30>&#39;t5&#39;</span>, bash_command<span style=color:#555>=</span><span style=color:#c30>&#39;echo &#34;task 5&#34;&#39;</span>, dag<span style=color:#555>=</span>dag)

<span style=color:#09f;font-style:italic># now arrange the tasks in the dag</span>
t2<span style=color:#555>.</span>set_upstream(t1)
t3<span style=color:#555>.</span>set_upstream(t1)
t4<span style=color:#555>.</span>set_upstream(t2)
t4<span style=color:#555>.</span>set_upstream(t3)
t5<span style=color:#555>.</span>set_upstream(t3)
</code></pre></div><p>Now that this is defined, you can instantiate a DAG Run from the webserver.</p><h2 id=notes-on-running-airflow>Notes On Running Airflow</h2><p>There have been a lot of <a href="https://news.ycombinator.com/item?id=17030102">justifiable claims against Airflow&rsquo;s deployment story</a>; it can be very frustrating. Scaling Airflow is confusing and fault-prone. For those who don&rsquo;t want to deal with the headache of managing the installation on their own, Google offers a <a href=https://cloud.google.com/composer/docs/concepts/features>hosted Airflow services called Cloud Composer</a>, or you can look into alternatives like Luigi or Mara. While I don&rsquo;t fully love Airflow, I still haven&rsquo;t found something better.</p><p>Headaches aside, it&rsquo;s still worth using a DAG engine because of the great improvements in both the quality and visibility of batch processing jobs.</p><h3 id=features-to-avoid>Features to Avoid</h3><p>Since Airflow has been in the Apache Incubator for the past year or so, it&rsquo;s been getting a lot of features rapidly. I choose to avoid these ones:</p><ul><li><a href=https://airflow.incubator.apache.org/concepts.html#xcoms>X-coms</a>. The biggest downside of Airflow compared to Luigi is that it doesn&rsquo;t force you to write idempotent tasks in the same way. It&rsquo;s easy to accidentally end up writing something that has outputs which don&rsquo;t reproduce. If you treat a task like a function with a single input and output, it works best.</li><li>Branching Operators - If you need to choose between multiple sets of logic, it&rsquo;s best just to dynamically set your task dependencies at runtime, as opposed to creating a branching task.</li><li>Bitshift Operators as Upstream/Downstream - Airflow recently introduced a way of setting upstream/downstream tasks by using bitshift operators like <code>&lt;&lt;</code> or <code>>></code>. This is bad. Explicit is good! <code>set_upstream()</code> is not particularly verbose.</li></ul><h2 id=in-conclusion>In Conclusion</h2><p>If you&rsquo;re frustrated with <code>cron</code> or other simple ways of deploying batch processes, give it a shot. Be aware of the shortcomings, though.</p></div></div></section></div></div></div><footer class=footer><div class="container has-text-centered"><p id=footer-text><strong>zmsy.co</strong> - A personal blog by Zach Morrissey. Copyright Â© 2022. Hosted via <a href=https://pages.github.com/>Github Pages</a>.</p><p><img class=footer-dots src=https://zmsy.co/img/footer.svg alt="Five horizontal hand-drawn dots."></p></div></footer><script type=text/javascript src=https://zmsy.co/ts/index.js></script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-104524581-1','auto'),ga('send','pageview'))</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>